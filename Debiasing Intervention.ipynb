{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from os import path, makedirs\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author: Diego Carraro, on the 01/04/2021\n",
    "contact: diego.carraro89@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version: 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Debiasing the Offline Evaluation of Recommender Systems</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is divided into three main sections:\n",
    "1. \"Setting of parameters for the evaluation\", where the user can tune the evaluation\n",
    "2. \"To prepare the data for the evaluation\", where the data for the evaluation is produced\n",
    "3. \"Methods\", that contains the methods used for the data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1 Setting of parameters for the evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = 'DatasetSample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# params for the splits and the intervention\n",
    "dataset_path = 'datasets/' + DATASET + '/'\n",
    "out_path = 'splits/' + DATASET + '/'\n",
    "do_validation = True # if to generate split to perform validation of hyperparameters\n",
    "\n",
    "n_splits = 2\n",
    "rho_test = 0.4 # heldout test proportion\n",
    "rho_p = 0.5 # intervention proportion\n",
    "rho_wei = 0.15 # proportion for the weights\n",
    "rho_val = 0.15 # validation proportion\n",
    "intervention_strategies = ['FULL', 'RND', 'SKEW', 'WTD', 'WTD_H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. To prepare the data for the evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAR_name = 'MAR_ratings'\n",
    "MNAR_name = 'MNAR_ratings'\n",
    "\n",
    "df_ratings_MAR = load_dataset(dataset_path, MAR_name, extension='.csv')\n",
    "df_ratings_MNAR = load_dataset(dataset_path, MNAR_name, extension='.csv')\n",
    "\n",
    "# filter MNAR dataset to have the same users of MAR dataset\n",
    "df_ratings_MNAR = df_ratings_MNAR.loc[df_ratings_MNAR['userId'].isin(df_ratings_MAR.userId.unique())]\n",
    "\n",
    "# retrieve list of users and items in the datasets\n",
    "d = list(set(df_ratings_MAR.userId.unique()).union(df_ratings_MNAR.userId.unique()))\n",
    "users = pd.DataFrame(data={'userId': d})\n",
    "d = list(set(df_ratings_MAR.itemId.unique()).union(df_ratings_MNAR.itemId.unique()))\n",
    "items = pd.DataFrame(data={'itemId': d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for fold in range(n_splits):\n",
    "    fold_dir = out_path + 'fold_' + str(fold) + '/'\n",
    "    if not path.exists(fold_dir):\n",
    "        makedirs(fold_dir)\n",
    "\n",
    "    users.to_csv(fold_dir + 'users.csv', index=False, sep='\\t', header=False)\n",
    "    items.to_csv(fold_dir + 'items.csv', index=False, sep='\\t', header=False)\n",
    "    \n",
    "    # split into training and heldout test\n",
    "    training, testing = simple_random_split(df_ratings_MNAR, rho_test)\n",
    "    training.to_csv(fold_dir + 'training.csv', index=False, sep='\\t', header=True)\n",
    "    testing.to_csv(fold_dir + 'heldout_test.csv', index=False, sep='\\t', header=True)\n",
    "    \n",
    "    # reserve a percentage of the MAR dataset for calculation of the weights for WTD\n",
    "    df_test_mar, df_weights_calculation = simple_random_split(df_ratings_MAR, rho_wei)\n",
    "    \n",
    "    # print the unbiased MAR ground truth test set\n",
    "    df_test_mar.to_csv(fold_dir + 'GT_test.csv', index=False, sep='\\t', header=False)\n",
    "\n",
    "    if do_validation:\n",
    "        df_test_mar, df_validation = simple_random_split(df_test_mar, rho_val)\n",
    "        df_validation.to_csv(fold_dir + 'validation_test.csv', index=False, sep='\\t', header=True)\n",
    "        \n",
    "    for strategy in intervention_strategies:\n",
    "        test_name = ''\n",
    "        intervened_test = None\n",
    "\n",
    "        if strategy == 'RND':\n",
    "            # random sampling the intervened test set\n",
    "            to_discard, intervened_test = simple_random_split(testing, rho_p)\n",
    "            test_name = 'RND_intervened_test.csv'\n",
    "\n",
    "        elif strategy == 'FULL':\n",
    "            # keep the entire test set\n",
    "            intervened_test = deepcopy(testing)\n",
    "            test_name = 'FULL_intervened_test.csv'\n",
    "\n",
    "        elif strategy == 'SKEW':\n",
    "            to_discard, intervened_test = skew_intervention(testing, training, set(items['itemId'].values), rho_p)\n",
    "            test_name = 'SKEW_intervened_test.csv'\n",
    "            \n",
    "        elif strategy == 'WTD' or strategy == 'WTD_H':\n",
    "            if strategy == 'WTD_H':\n",
    "                use_ideal_mar_distr = True\n",
    "            else:\n",
    "                use_ideal_mar_distr = False\n",
    "\n",
    "            user_weights, item_weights = calculate_weights(training, df_weights_calculation,\n",
    "                                                           use_ideal_mar_distr,\n",
    "                                                           set(users['userId'].values),\n",
    "                                                           set(items['itemId'].values))\n",
    "            \n",
    "            to_discard, intervened_test = weighted_intervention(testing, user_weights, item_weights, rho_p)\n",
    "            test_name = strategy + '_intervened_test.csv'\n",
    "\n",
    "        # write it\n",
    "        intervened_test.to_csv(fold_dir + test_name, index=False, sep='\\t', header=True)\n",
    "\n",
    "    print(\"time elapsed\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(pathh, name, extension='.csv'):\n",
    "    ratings = pd.read_csv(pathh + name + extension, sep='\\t')\n",
    "    print('dataset loaded')\n",
    "    return ratings\n",
    "\n",
    "def simple_random_split(df, frac):\n",
    "    df_test = df.copy()\n",
    "    n_ratings = len(df_test)\n",
    "    test_idx = np.random.choice(n_ratings, size=int(frac * n_ratings), replace=False)\n",
    "\n",
    "    mask = np.zeros(n_ratings, dtype=bool)\n",
    "    mask[test_idx] = True\n",
    "\n",
    "    test = df_test[mask]\n",
    "    train = df_test[~mask]\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skew_intervention(df_testing, df_training, all_items, frac):\n",
    "    # calculate pop scores\n",
    "    items_pop = df_training['itemId'].value_counts(sort=True, ascending=True, normalize=False)\n",
    "    pop_dict = items_pop.to_dict()\n",
    "    pop_dict = {k: 1 / v for k, v in pop_dict.items()}\n",
    "\n",
    "    # for items not included in the training set, set a random probability\n",
    "    not_in_training = all_items.difference(pop_dict.keys())\n",
    "    for ii in not_in_training:\n",
    "        v = random.random()\n",
    "        pop_dict[ii] = v\n",
    "\n",
    "    df_test = df_testing.copy()\n",
    "    n_ratings = len(df_testing)\n",
    "\n",
    "    df_test['p_item'] = df_test['itemId'].apply(lambda x: pop_dict[x])\n",
    "    # normalize the probabilities\n",
    "    p_sum = df_test['p_item'].sum()\n",
    "    df_test['p_item'] = df_test['p_item'].apply(lambda x: x / p_sum)\n",
    "\n",
    "    test_idx = np.random.choice(n_ratings, size=int(frac * n_ratings), replace=False, p=df_test['p_item'].values)\n",
    "    mask = np.zeros(n_ratings, dtype=bool)\n",
    "    mask[test_idx] = True\n",
    "\n",
    "    del df_test['p_item']\n",
    "    test = df_test[mask]\n",
    "    train = df_test[~mask]\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_intervention(df_testing, user_w, item_w, frac):\n",
    "\n",
    "    # calculate P(S|u,i,w) for every pair in df_testing\n",
    "    p_s = np.zeros(len(df_testing))\n",
    "\n",
    "    for ix, row in df_testing.iterrows():\n",
    "        p_s[ix] = user_w[row['userId']] * item_w[row['itemId']]\n",
    "    \n",
    "    # normalize p_s\n",
    "    tot = sum(p_s)\n",
    "    p_s = np.array(p_s) / tot\n",
    "\n",
    "    # perform the sampling with P(S|u,i,w)\n",
    "    df_test = df_testing.copy()\n",
    "    n_ratings = len(df_test)\n",
    "    test_idx = np.random.choice(n_ratings, size=int(frac * n_ratings), replace=False, p=p_s)\n",
    "\n",
    "    mask = np.zeros(n_ratings, dtype=bool)\n",
    "    mask[test_idx] = True\n",
    "\n",
    "    test = df_test[mask]\n",
    "    train = df_test[~mask]\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_weights(df_training, df_mar, use_ideal, usrs, itms):\n",
    "    # calculate P(u|O) for MAR\n",
    "    p_u_O_MAR = calculate_p_u_o(df_mar, usrs, use_ideal)\n",
    "\n",
    "    # calculate P(u|O) for MNAR\n",
    "    p_u_O_MNAR = calculate_p_u_o(df_training, usrs, False)\n",
    "\n",
    "    # calculate P(i|O) for MAR\n",
    "    p_i_O_MAR = calculate_p_i_o(df_mar, itms, use_ideal)\n",
    "\n",
    "    # calculate P(i|O) for MNAR\n",
    "    p_i_O_MNAR = calculate_p_i_o(df_training, itms, False)\n",
    "\n",
    "    w_users = initialize_weights(p_u_O_MNAR.keys())\n",
    "    w_items = initialize_weights(p_i_O_MNAR.keys())\n",
    "\n",
    "    w_users = exact_weight_calculation(p_u_O_MAR, p_u_O_MNAR, w_users)\n",
    "    w_items = exact_weight_calculation(p_i_O_MAR, p_i_O_MNAR, w_items)\n",
    "\n",
    "    # normalize weights\n",
    "    w_users = norm(w_users)\n",
    "    w_items = norm(w_items)\n",
    "\n",
    "    return w_users, w_items\n",
    "\n",
    "def initialize_weights(list_of):\n",
    "    w = {}\n",
    "    for e in list_of:\n",
    "        w[e] = random.random()\n",
    "    return w\n",
    "\n",
    "def exact_weight_calculation(p_mar, p_mnar, gd_w):\n",
    "    for ii in gd_w:\n",
    "        gd_w[ii] = p_mar[ii] / p_mnar[ii]\n",
    "    return gd_w\n",
    "\n",
    "\n",
    "def norm(vect):\n",
    "    sum_vect = sum(list(vect.values()))\n",
    "    for key in vect:\n",
    "        vect[key] = vect[key] / sum_vect\n",
    "    return vect\n",
    "\n",
    "def calculate_p_u_o(df, usrs, use_ideal):\n",
    "    n = len(df)\n",
    "    ideal_distr = 1 / len(usrs)\n",
    "    p_u_o = {}\n",
    "    for user in usrs:\n",
    "        if use_ideal:\n",
    "            p_u_o[user] = ideal_distr\n",
    "        else:\n",
    "            p_u_o[user] = len(df[df['userId'] == user]) / n\n",
    "        if p_u_o[user] == 0:\n",
    "            p_u_o[user] = 0.0001\n",
    "    return p_u_o\n",
    "\n",
    "\n",
    "def calculate_p_i_o(df, itms, use_ideal):\n",
    "    n = len(df)\n",
    "    ideal_distr = 1 / len(itms)\n",
    "    p_i_o = {}\n",
    "    for item in itms:\n",
    "        if use_ideal:\n",
    "            p_i_o[item] = ideal_distr\n",
    "        else:\n",
    "            p_i_o[item] = len(df[df['itemId'] == item]) / n\n",
    "        if p_i_o[item] == 0:\n",
    "            p_i_o[item] = 0.0001\n",
    "    return p_i_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
